{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08214048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the directory containing lingpred to sys.path\n",
    "sys.path.append(os.path.abspath(\"/Users/xiaosuhu/Python_Proj/Pilot_Analysis_3_Way_Split/lingpred/lingpred\"))\n",
    "# sys.path.append(os.path.abspath(\"F:\\Matlab_Project\\PROJECT_HH_ENGLISH\\Pilot_Analysis_3_Way_Split\\lingpred\\lingpred\"))\n",
    "\n",
    "from lingpred.lingpred.pipeline import LingPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV file\n",
    "df = pd.read_csv(\"./DesignMatrix/HH_time_matrix.csv\")  # Replace with the actual filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the 'Text' column\n",
    "text_column = df[\"Text\"]\n",
    "\n",
    "# Combine into a single string\n",
    "cleaned_text = \" \".join(df[\"Text\"].astype(str).str.strip())\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e85772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is used for testing the limit of the words can be used.\n",
    "# Split the cleaned text into words\n",
    "words = cleaned_text.split()\n",
    "\n",
    "# Take the first 100 words\n",
    "first_100_words = \" \".join(words[:201])\n",
    "\n",
    "print(first_100_words)\n",
    "# Can not put the full text of 2000 word in as it exceed the limit\n",
    "lp = LingPred()\n",
    "lp.preprocess(first_100_words) # Load text\n",
    "lp.fit() # Run GPT2-Small FFN\n",
    "lp.get_lexical_info() # Get Lexical surprisals\n",
    "lp.get_POS_info() # Get POS labels and surprisals\n",
    "lp.get_phonemic_info() # Get phonemic labels and phonemic susprisals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_last(arr):\n",
    "    \"\"\"Return last element of a list-like or None.\"\"\"\n",
    "    try:\n",
    "        return arr[-1] if arr is not None and len(arr) > 0 else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _safe_float(x):\n",
    "    \"\"\"Convert numpy/Python numbers to float, map inf/-inf to None.\"\"\"\n",
    "    try:\n",
    "        if x is None:\n",
    "            return None\n",
    "        xf = float(x)\n",
    "        if math.isfinite(xf):\n",
    "            return xf\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def compute_suprisal_over_story(\n",
    "    cleaned_text: str,\n",
    "    window_words: int = 200,\n",
    "    save_path: str = None,\n",
    "    verbose_every: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling-context surprisal over entire story.\n",
    "\n",
    "    For each word i, we run LingPred on the preceding `window_words` (or fewer)\n",
    "    PLUS the target word, then record the final entry from lp.out_dict arrays.\n",
    "    \"\"\"\n",
    "    words_all = cleaned_text.split()\n",
    "    n = len(words_all)\n",
    "    # n=10 # for test purpose\n",
    "    rows = []\n",
    "    lp = LingPred()\n",
    "\n",
    "    for i in trange(n, desc=\"Computing surprisals\"):\n",
    "        start = max(0, i - window_words)\n",
    "        chunk_words = words_all[start : i + 1]       # include target\n",
    "        text_chunk = \" \".join(chunk_words)\n",
    "\n",
    "        try:\n",
    "            # Run the pipeline for this chunk\n",
    "            lp.preprocess(text_chunk)\n",
    "            lp.fit()\n",
    "            lp.get_lexical_info()\n",
    "            lp.get_POS_info()\n",
    "            lp.get_phonemic_info()\n",
    "\n",
    "            # Grab arrays from out_dict (defensive .get in case keys vary)\n",
    "            words_chunk             = getattr(lp, \"words\", chunk_words)\n",
    "            lexical_surprise_arr    = lp.out_dict.get(\"lexical_surprise\")\n",
    "            pos_surprise_arr        = lp.out_dict.get(\"POS_surprise\")\n",
    "            content_labels_arr      = lp.out_dict.get(\"content_words\")\n",
    "            function_labels_arr     = lp.out_dict.get(\"function_words\")\n",
    "            phoneme_initial_arr     = lp.out_dict.get(\"phoneme\")\n",
    "            phonemic_surprise_arr   = lp.out_dict.get(\"phonemic_surprise\")\n",
    "\n",
    "            # The target is the *last* element of each array\n",
    "            target_word             = _safe_last(words_chunk) or words_all[i]\n",
    "            lex_surprisal           = _safe_float(_safe_last(lexical_surprise_arr))\n",
    "            pos_surprisal           = _safe_float(_safe_last(pos_surprise_arr))\n",
    "            is_content              = _safe_last(content_labels_arr)\n",
    "            is_function             = _safe_last(function_labels_arr)\n",
    "            phoneme_initial         = _safe_last(phoneme_initial_arr)\n",
    "            phoneme_surprisal       = _safe_float(_safe_last(phonemic_surprise_arr))\n",
    "\n",
    "            rows.append({\n",
    "                \"orig_index\": i,                         # index in full story\n",
    "                \"word\": words_all[i],                    # original token\n",
    "                \"model_word\": target_word,               # last word seen by model in this chunk\n",
    "                \"lex_surprisal\": lex_surprisal,\n",
    "                \"pos_surprisal\": pos_surprisal,\n",
    "                \"is_content_word\": bool(is_content) if is_content is not None else None,\n",
    "                \"is_function_word\": bool(is_function) if is_function is not None else None,\n",
    "                \"phoneme_initial\": phoneme_initial,\n",
    "                \"phoneme_surprisal\": phoneme_surprisal,\n",
    "                \"window_start_index\": start,\n",
    "                \"window_len_words\": len(chunk_words),\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # Keep going on errors; record Nones to preserve alignment\n",
    "            if (i % verbose_every) == 0:\n",
    "                print(f\"[warn] i={i}, error: {e}\")\n",
    "            rows.append({\n",
    "                \"orig_index\": i,\n",
    "                \"word\": words_all[i],\n",
    "                \"model_word\": None,\n",
    "                \"lex_surprisal\": None,\n",
    "                \"pos_surprisal\": None,\n",
    "                \"is_content_word\": None,\n",
    "                \"is_function_word\": None,\n",
    "                \"phoneme_initial\": None,\n",
    "                \"phoneme_surprisal\": None,\n",
    "                \"window_start_index\": start,\n",
    "                \"window_len_words\": len(chunk_words),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if save_path:\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Saved results to {save_path}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example use:\n",
    "df_surprisal = compute_suprisal_over_story(\n",
    "    cleaned_text,\n",
    "    window_words=200,\n",
    "    save_path=\"story_surprisal_2.csv\"\n",
    ")\n",
    "\n",
    "df_surprisal.to_csv(\"HH_surprisal_3_way.csv\", index=False)\n",
    "df_surprisal.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316052db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fnirs_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
